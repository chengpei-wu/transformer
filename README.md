# Transformer implementation in PyTorch

- paper: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- pytorch official implementation: https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html

## Dependencies

- Pytorch

## Implementation of key blocks

- positional encoding
- attention
- multi head attention
- layer normalization
- feedforward nn
- transformer encoder
- transformer decoder
- transformer encoder layer
- transformer decoder layer

## Acknowledgement

Thanks for the tutorial [annotated-transformer tutorial](https://github.com/harvardnlp/annotated-transformer).
